# -*- coding: utf-8 -*-
"""RSS parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d7RtWOsIJadkwXgNYe4IbnnCuPbshuzT
"""

import feedparser
import re
from bs4 import BeautifulSoup

def parse_rss_feed(rss_url):
    """
    Parse an RSS feed and extract specific fields from each item.

    Args:
        rss_url (str): The URL of the RSS feed to parse

    Returns:
        list: A list of dictionaries containing the extracted fields
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    extracted_data = []

    # Iterate through each item in the feed
    for entry in feed.entries:
        item_data = {
            "title": clean_text(entry.get('title', '')),
            "link": entry.get('link', ''),
            "description": clean_html(entry.get('description', '')),
            "pub_date": entry.get('published', ''),
            "content": clean_html(get_content(entry))
        }
        extracted_data.append(item_data)

    return extracted_data

def get_content(entry):
    """
    Extract content from RSS entry, prioritizing content module if available.

    Args:
        entry: A feedparser entry object

    Returns:
        str: The content text
    """
    # Try to get content from content module first
    if hasattr(entry, 'content'):
        for content in entry.content:
            if hasattr(content, 'value'):
                return content.value

    # Fallback to description if content is not available
    return entry.get('description', '')

def clean_html(html_text):
    """
    Remove HTML tags and clean the text.

    Args:
        html_text (str): Text containing HTML tags

    Returns:
        str: Clean text without HTML tags
    """
    if not html_text:
        return ""

    # Use BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(html_text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Remove extra whitespace and clean up
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()

    return clean_text

def clean_text(text):
    """
    Basic text cleaning (remove extra whitespace, etc.)

    Args:
        text (str): Text to clean

    Returns:
        str: Cleaned text
    """
    if not text:
        return ""

    return re.sub(r'\s+', ' ', text).strip()

# Example usage
if __name__ == "__main__":
    # Extract the RSS URL from the snippet
    rss_url = "https://nawaat.org/feed/"

    # Parse the feed
    data = parse_rss_feed(rss_url)

    # Print the extracted data in a clean format
    for i, item in enumerate(data, 1):
        print(f"Item {i}:")
        print(f"  Title: {item['title']}")
        print(f"  Link: {item['link']}")
        print(f"  Description: {(item['description'])}")
        print(f"  Publication Date: {item['pub_date']}")
        print(f"  Content: {(item['content'])}")
        print("-" * 80)

def truncate_text(text, max_length):
    """
    Truncate text to specified length and add ellipsis if needed.

    Args:
        text (str): Text to truncate
        max_length (int): Maximum length before truncation

    Returns:
        str: Truncated text
    """
    if len(text) <= max_length:
        return text
    return text[:max_length].rsplit(' ', 1)[0] + '...'

import feedparser
from bs4 import BeautifulSoup
import html

def parse_rss_feed(url):
    """
    Parse RSS feed and extract required fields with HTML cleaning
    """
    # Parse the RSS feed
    feed = feedparser.parse(url)

    extracted_data = []

    for entry in feed.entries:
        # Extract basic fields
        title = getattr(entry, 'title', '')
        link = getattr(entry, 'link', '')
        pub_date = getattr(entry, 'published', getattr(entry, 'pubDate', ''))

        # Extract and clean description
        description = getattr(entry, 'description', '')
        description = clean_html_content(description)

        # Extract content (try multiple possible fields)
        content = getattr(entry, 'content', [{}])[0].get('value', '') if hasattr(entry, 'content') else ''
        if not content:
            content = getattr(entry, 'summary', '')
        content = clean_html_content(content)

        # Create entry dictionary
        entry_data = {
            "title": clean_text(title),
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        extracted_data.append(entry_data)

    return extracted_data

def clean_html_content(html_content):
    """
    Clean HTML content using BeautifulSoup to extract plain text
    """
    if not html_content:
        return ""

    try:
        # Parse HTML content
        soup = BeautifulSoup(html_content, 'html.parser')

        # Remove unwanted tags but keep text content
        for element in soup(['script', 'style', 'img', 'iframe', 'form', 'input', 'button']):
            element.decompose()

        # Get clean text
        clean_text = soup.get_text(separator=' ', strip=True)

        # Decode HTML entities and clean up whitespace
        clean_text = html.unescape(clean_text)
        clean_text = ' '.join(clean_text.split())

        return clean_text

    except Exception as e:
        print(f"Error cleaning HTML content: {e}")
        # Fallback: return original content with basic cleaning
        return clean_text(html_content)

def clean_text(text):
    """
    Basic text cleaning for non-HTML fields
    """
    if not text:
        return ""

    text = html.unescape(text)
    text = ' '.join(text.split())
    return text

# Example usage with the provided RSS feed URL
if __name__ == "__main__":
    rss_url = "http://assarih.com/feed/"

    try:
        data = parse_rss_feed(rss_url)

        # Print extracted data
        for i, entry in enumerate(data, 1):
            print(f"Entry {i}:")
            print(f"Title: {entry['title']}")
            print(f"Link: {entry['link']}")
            print(f"Description: {entry['description']}")
            print(f"Publication Date: {entry['pub_date']}")
            print(f"Content: {entry['content']}")
            print("-" * 80)

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import re

def extract_rss_feed_data(rss_url):
    """
    Extract and clean RSS feed data from the given URL
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    extracted_data = []

    for entry in feed.entries:
        # Extract basic fields
        title = entry.get('title', '')
        link = entry.get('link', '')
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Clean description using BeautifulSoup
        description_html = entry.get('description', '')
        description_clean = clean_html(description_html)
        description_clean = remove_boilerplate(description_clean, title)

        # Extract content (try multiple possible fields)
        content_html = entry.get('content', [{}])[0].get('value', '') if 'content' in entry else ''
        if not content_html:
            content_html = entry.get('content:encoded', '')

        content_clean = clean_html(content_html)
        content_clean = remove_boilerplate(content_clean, title)

        # Create data dictionary
        item_data = {
            "title": title,
            "link": link,
            "description": description_clean,
            "pub_date": pub_date,
            "content": content_clean
        }

        extracted_data.append(item_data)

    return extracted_data

def clean_html(html_content):
    """
    Clean HTML content using BeautifulSoup to extract text only
    """
    if not html_content:
        return ""

    # Parse HTML with BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')

    # Remove unwanted tags (script, style, etc.)
    for unwanted_tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
        unwanted_tag.decompose()

    # Get clean text
    clean_text = soup.get_text(separator=' ', strip=True)

    # Clean up extra whitespace
    clean_text = ' '.join(clean_text.split())

    return clean_text

def remove_boilerplate(text, title):
    """
    Remove repetitive boilerplate text from the content
    """
    if not text:
        return ""

    # Pattern to match the repetitive boilerplate text
    patterns = [
        r"L'article.*est apparu en premier sur WMC\..*$",
        r"L'article.*est apparu en premier sur WMC.*$",
        r"est apparu en premier sur WMC\..*$",
        r"est apparu en premier sur WMC.*$"
    ]

    # Try each pattern to remove boilerplate
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)

    # Also remove any trailing "L'article" references that might be left
    text = re.sub(r"L'article.*$", '', text)

    # Clean up any extra whitespace created by the removal
    text = text.strip()

    # Remove any trailing ellipsis or incomplete sentences
    text = re.sub(r'\[…\]$|\.\.\.$|…$', '', text).strip()
    text = re.sub(r'\[\…\]$', '', text).strip()

    return text

# Example usage
if __name__ == "__main__":
    rss_url = "https://www.webmanagercenter.com/feed/"

    try:
        # Extract data from RSS feed
        data = extract_rss_feed_data(rss_url)

        # Print the extracted data
        for i, item in enumerate(data, 1):
            print(f"Item {i}:")
            print(f"Title: {item['title']}")
            print(f"Link: {item['link']}")
            print(f"Description: {item['description']}")
            print(f"Publication Date: {item['pub_date']}")
            print(f"Content: {item['content']}")
            print("-" * 80)

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean the text using BeautifulSoup"""
    if not text:
        return ""

    # Parse with BeautifulSoup
    soup = BeautifulSoup(text, 'html.parser')

    # Get clean text
    clean_text = soup.get_text(separator=' ', strip=False)

    # Clean up any remaining HTML entities and extra whitespace
    clean_text = re.sub(r'\s+', ' ', clean_text)  # Replace multiple spaces with single space
    clean_text = clean_text.strip()

    return clean_text

def extract_rss_feed_data(url):
    """Extract and clean data from RSS feed"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    results = []

    for entry in feed.entries:
        # Extract the required fields
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')

        # Handle description (could be in different fields)
        description = clean_html_content(entry.get('description', ''))
        if not description:
            description = clean_html_content(entry.get('summary', ''))

        # Handle publication date
        pub_date = entry.get('published', '')
        if not pub_date:
            pub_date = entry.get('pubDate', '')

        # Handle content (could be in different fields)
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value)
        elif hasattr(entry, 'content_encoded'):
            content = clean_html_content(entry.content_encoded)
        elif hasattr(entry, 'description'):
            # Fallback to description if no specific content field
            content = clean_html_content(entry.description)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

# Extract the RSS URL from the snippet
rss_url = "https://www.leconomistemaghrebin.com/feed/"

# Parse and extract data
try:
    extracted_data = extract_rss_feed_data(rss_url)

    # Print the extracted data without truncation
    for i, item in enumerate(extracted_data, 1):
        print(f"Item {i}:")
        print(f"Title: {item['title']}")
        print(f"Link: {item['link']}")
        print(f"Description: {item['description']}")
        print(f"Publication Date: {item['pub_date']}")
        print(f"Content: {item['content']}")
        print("-" * 80)

except Exception as e:
    print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html
import re

def clean_html_content(text):
    """
    Remove HTML tags and clean text content using BeautifulSoup
    """
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities and clean up whitespace
    clean_text = html.unescape(clean_text)
    clean_text = ' '.join(clean_text.split())

    return clean_text

def extract_description_and_content(text):
    """
    Extract description (after "The post") and content (before "[...]") from HTML text
    """
    # Parse the HTML content
    soup = BeautifulSoup(text, 'html.parser')

    # Extract all text content
    full_text = soup.get_text(separator=' ', strip=True)

    # Extract content before "[...]" - look for the main content
    content_match = re.search(r'^(.*?)\s*\[\.\.\.\]', full_text)
    content = content_match.group(1).strip() if content_match else full_text

    # Extract description from the "The post" line
    description_match = re.search(r'The post\s+(.*?)\s+appeared first', full_text)
    if description_match:
        description = description_match.group(1).strip()
    else:
        # Fallback: if no "The post" pattern found, try to get the title from the link
        post_link = soup.find('a', rel='nofollow')
        if post_link:
            description = post_link.get_text(strip=True)
        else:
            # Final fallback: use first part of content
            words = content.split()
            description = ' '.join(words[:min(10, len(words))]) + ('...' if len(words) > 10 else '')

    return description, content

def extract_rss_feed_data(url):
    """
    Extract and clean data from RSS feed
    """
    # Parse the RSS feed
    feed = feedparser.parse(url)

    extracted_data = []

    for entry in feed.entries:
        # Extract basic fields
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Get the raw description (HTML content)
        raw_description = entry.get('description', '')

        # Extract description and content from the HTML description field
        description, content = extract_description_and_content(raw_description)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        extracted_data.append(result)

    return extracted_data

# Example usage with the RSS URL from your snippet
if __name__ == "__main__":
    rss_url = "https://radioexpressfm.com/fr/feed/"

    try:
        # Extract data from RSS feed
        data = extract_rss_feed_data(rss_url)

        # Print the extracted data without truncation
        for i, item in enumerate(data, 1):
            print(f"Item {i}:")
            print(f"Title: {item['title']}")
            print(f"Link: {item['link']}")
            print(f"Description: {item['description']}")
            print(f"Publication Date: {item['pub_date']}")
            print(f"Content: {item['content']}")
            print("-" * 80)

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse HTML content
    soup = BeautifulSoup(text, 'html.parser')

    # Remove unwanted tags but keep text content
    for element in soup(['script', 'style', 'img', 'iframe', 'form', 'input', 'button']):
        element.decompose()

    # Get clean text and unescape HTML entities
    clean_text = soup.get_text(separator=' ', strip=True)
    clean_text = html.unescape(clean_text)

    # Remove extra whitespace
    clean_text = ' '.join(clean_text.split())

    return clean_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    feed = feedparser.parse(rss_url)

    articles = []

    for entry in feed.entries:
        article = {
            "title": clean_html_content(entry.get('title', '')),
            "link": entry.get('link', ''),
            "description": clean_html_content(entry.get('description', '')),
            "pub_date": entry.get('published', entry.get('pubDate', '')),
            "content": clean_html_content(entry.get('content', [{}])[0].get('value', '')) if entry.get('content') else clean_html_content(entry.get('description', ''))
        }
        articles.append(article)

    return articles

def main():
    # RSS feed URL from the snippet
    rss_url = "https://realites.com.tn/feed/"

    try:
        articles = parse_rss_feed(rss_url)

        for i, article in enumerate(articles, 1):
            print(f"=== Article {i} ===")
            print(f"Title: {article['title']}")
            print(f"Link: {article['link']}")
            print(f"Description: {article['description']}")
            print(f"Publication Date: {article['pub_date']}")
            print(f"Content: {article['content']}")
            print("\n" + "-" * 80 + "\n")

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

if __name__ == "__main__":
    main()

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse HTML content
    soup = BeautifulSoup(text, 'html.parser')

    # Get clean text without HTML tags
    clean_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities
    clean_text = html.unescape(clean_text)

    return clean_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields with swapped description/content"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    # Extract information from each item
    for entry in feed.entries:
        # Get the raw values before cleaning
        raw_title = entry.get('title', '')
        raw_description = entry.get('description', '')

        # Clean the content
        clean_title = clean_html_content(raw_title)
        clean_description = clean_html_content(raw_description)

        # For content field, try multiple sources and use description if content is empty or same as title
        content_candidates = []

        # Try content field first
        if entry.get('content'):
            content_value = clean_html_content(entry.get('content', [{}])[0].get('value', ''))
            if content_value and content_value != clean_title:
                content_candidates.append(content_value)

        # Try description as content (if it's different from title)
        if clean_description and clean_description != clean_title:
            content_candidates.append(clean_description)

        # Try summary field
        if entry.get('summary'):
            summary_value = clean_html_content(entry.get('summary', ''))
            if summary_value and summary_value != clean_title:
                content_candidates.append(summary_value)

        # Use the first available content candidate, or fallback to description
        final_content = content_candidates[0] if content_candidates else clean_description

        # For description, use title if content was taken from description
        final_description = clean_title if final_content == clean_description else clean_description

        item_data = {
            "title": clean_title,
            "link": entry.get('link', ''),
            "description": final_description,
            "pub_date": entry.get('published', entry.get('pubDate', '')),
            "content": final_content
        }
        results.append(item_data)

    return results

# RSS URL from the snippet
rss_url = "https://www.radiotunisienne.tn/articles/rss"

# Parse the RSS feed
parsed_data = parse_rss_feed(rss_url)

# Print the extracted data without truncation
for i, item in enumerate(parsed_data, 1):
    print(f"Item {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)
    print()

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean the text using BeautifulSoup"""
    if not text:
        return ""

    # Parse with BeautifulSoup
    soup = BeautifulSoup(text, 'html.parser')

    # Get clean text
    clean_text = soup.get_text(separator=' ', strip=True)

    # Remove extra whitespace and normalize
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()

    return clean_text

def extract_rss_feed_data(rss_url):
    """Extract and clean data from RSS feed"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    extracted_data = []

    # Process each item in the feed
    for entry in feed.entries:
        # Get description content
        description = clean_html_content(entry.get('description', ''))

        # Get content - if not available, use description as fallback
        content = ''
        if entry.get('content'):
            content = clean_html_content(entry.get('content', [{}])[0].get('value', ''))
        else:
            content = description  # Use description as content

        item_data = {
            "title": clean_html_content(entry.get('title', '')),
            "link": entry.get('link', ''),
            "description": description,
            "pub_date": entry.get('published', entry.get('pubDate', '')),
            "content": content
        }
        extracted_data.append(item_data)

    return extracted_data

# RSS URL from the snippet
rss_url = "https://www.leaders.com.tn/rss"

try:
    # Extract data from the RSS feed
    feed_data = extract_rss_feed_data(rss_url)

    # Print the extracted data without truncation
    for i, item in enumerate(feed_data, 1):
        print(f"Item {i}:")
        print(f"Title: {item['title']}")
        print(f"Link: {item['link']}")
        print(f"Description: {item['description']}")
        print(f"Publication Date: {item['pub_date']}")
        print(f"Content: {item['content']}")
        print("-" * 80)

except Exception as e:
    print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and decode HTML entities from text"""
    if not text:
        return ""

    # Decode HTML entities first
    cleaned_text = html.unescape(text)

    # Remove HTML tags using BeautifulSoup
    soup = BeautifulSoup(cleaned_text, 'html.parser')
    cleaned_text = soup.get_text(separator=' ', strip=True)

    return cleaned_text

def extract_rss_feed_data(rss_url):
    """Extract and clean data from RSS feed"""

    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    extracted_data = []

    for entry in feed.entries:
        # Extract and clean each field
        item_data = {
            "title": clean_html_content(entry.get('title', '')),
            "link": entry.get('link', ''),
            "description": clean_html_content(entry.get('description', '')),
            "pub_date": entry.get('published', entry.get('pubDate', '')),
            "content": clean_html_content(entry.get('content', [{}])[0].get('value', '')) if entry.get('content') else ''
        }

        # If content is empty, try other common content fields
        if not item_data['content']:
            item_data['content'] = clean_html_content(
                entry.get('summary', '') or
                entry.get('content:encoded', '') or
                entry.get('description', '')
            )

        extracted_data.append(item_data)

    return extracted_data

# Example usage with the provided RSS URL
rss_url = "https://africanmanager.com/feed/"

try:
    data = extract_rss_feed_data(rss_url)

    for i, item in enumerate(data, 1):
        print(f"Item {i}:")
        print(f"Title: {item['title']}")
        print(f"Link: {item['link']}")
        print(f"Description: {item['description']}")
        print(f"Publication Date: {item['pub_date']}")
        print(f"Content: {item['content']}")
        print("-" * 80)
        print()

except Exception as e:
    print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html
import re

def extract_rss_fields(rss_url):
    """
    Extract and clean RSS feed fields using feedparser and BeautifulSoup
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = getattr(entry, 'title', '')
        link = getattr(entry, 'link', '')
        pub_date = getattr(entry, 'published', getattr(entry, 'pubDate', ''))

        # Extract and clean description - remove author and date information
        description = getattr(entry, 'description', '')
        description = clean_description(description)

        # For content, use the cleaned description (since content field may not be available)
        content = description

        # Create result dictionary
        result = {
            "title": clean_text(title),
            "link": clean_text(link),
            "description": description,
            "pub_date": clean_text(pub_date),
            "content": content
        }

        results.append(result)

    return results

def clean_description(description):
    """
    Clean description by extracting only the main text content
    and removing author and date information
    """
    if not description:
        return ""

    try:
        # Parse HTML content
        soup = BeautifulSoup(description, 'html.parser')

        # Find the span with property="schema:name" which contains the main title
        title_span = soup.find('span', {'property': 'schema:name'})

        if title_span:
            # Extract text from the title span
            main_text = title_span.get_text(strip=True)
            return clean_text(main_text)
        else:
            # Fallback: get all text and try to extract the main content
            all_text = soup.get_text(separator=' ', strip=True)
            # Remove author and date information (simple pattern matching)
            # This regex removes everything after the main title text
            cleaned_text = re.sub(r'(\s+\w+_\w+\s+.*|\s+ven\s+\d{2}/\d{2}/\d{4}.*)', '', all_text)
            return clean_text(cleaned_text)

    except Exception as e:
        print(f"Error cleaning description: {e}")
        return clean_text(description)

def clean_text(text):
    """
    Clean plain text by unescaping HTML entities and normalizing whitespace
    """
    if not text:
        return ""

    try:
        # Unescape HTML entities
        text = html.unescape(text)

        # Normalize whitespace
        text = ' '.join(text.split())

        return text.strip()
    except Exception as e:
        print(f"Error cleaning text: {e}")
        return str(text)

def print_results(results):
    """
    Print the extracted results in the exact format requested
    """
    for i, result in enumerate(results, 1):
        print(f"=== Entry {i} ===")
        print(f"Title: {result['title']}")
        print(f"Link: {result['link']}")
        print(f"Description: {result['description']}")
        print(f"Publication Date: {result['pub_date']}")
        print(f"Content: {result['content']}")
        print("=" * 50)
        print()

# Example usage with the provided RSS snippet URL
if __name__ == "__main__":
    # The URL from the RSS snippet
    rss_url = "https://www.alchourouk.com/rss"

    # Extract fields from the RSS feed
    extracted_data = extract_rss_fields(rss_url)

    # Print the results
    print_results(extracted_data)

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_feed_content(rss_url):
    """
    Extract and clean RSS feed content from the given URL
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = getattr(entry, 'title', '')
        link = getattr(entry, 'link', '')
        pub_date = getattr(entry, 'published', getattr(entry, 'pubDate', ''))

        # Extract and clean description
        description = getattr(entry, 'description', '')
        description = clean_html_content(description)

        # Extract and clean content (try multiple possible fields)
        content = ''
        if hasattr(entry, 'content'):
            content = entry.content[0].value if entry.content else ''
        elif hasattr(entry, 'content_encoded'):
            content = entry.content_encoded
        elif hasattr(entry, 'summary'):
            content = entry.summary

        content = clean_html_content(content)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

def clean_html_content(text):
    """
    Clean HTML content using BeautifulSoup and handle HTML entities
    """
    if not text:
        return ""

    # Decode HTML entities first
    text = html.unescape(text)

    # Use BeautifulSoup to extract text content
    soup = BeautifulSoup(text, 'html.parser')

    # Get clean text without HTML tags
    clean_text = soup.get_text(separator=' ', strip=False)

    # Remove extra whitespace but preserve paragraph structure
    clean_text = ' '.join(clean_text.split())

    return clean_text

def print_extracted_content(results):
    """
    Print the extracted content without truncation
    """
    for i, result in enumerate(results, 1):
        print(f"=== Entry {i} ===")
        print(f"Title: {result['title']}")
        print(f"Link: {result['link']}")
        print(f"Description: {result['description']}")
        print(f"Publication Date: {result['pub_date']}")
        print(f"Content: {result['content']}")
        print("\n" + "="*50 + "\n")

# Main execution
if __name__ == "__main__":
    rss_url = "https://realites.com.tn/feed/"

    try:
        extracted_data = extract_rss_feed_content(rss_url)
        print_extracted_content(extracted_data)
    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text using BeautifulSoup"""
    if not text:
        return ""

    # Parse HTML content
    soup = BeautifulSoup(text, 'html.parser')

    # Remove unwanted elements (social media buttons, scripts, styles, etc.)
    for element in soup.find_all(['script', 'style', 'a', 'div', 'span', 'class']):
        if 'a2a_button' in str(element.get('class', [])):
            element.decompose()

    # Get clean text
    clean_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities
    clean_text = html.unescape(clean_text)

    # Remove extra whitespace
    clean_text = ' '.join(clean_text.split())

    return clean_text

def extract_rss_feed_data(url):
    """Extract and clean data from RSS feed"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')

        # Handle description (could be in different fields)
        description = ''
        if hasattr(entry, 'description'):
            description = clean_html_content(entry.description)
        elif hasattr(entry, 'summary'):
            description = clean_html_content(entry.summary)

        # Handle publication date
        pub_date = entry.get('published', '') or entry.get('pubDate', '') or entry.get('updated', '')

        # Handle content (could be in different fields)
        content = ''
        if hasattr(entry, 'content'):
            # If multiple content entries, take the first one
            if entry.content:
                content = clean_html_content(entry.content[0].value)
        elif hasattr(entry, 'content:encoded'):
            content = clean_html_content(entry.get('content:encoded', ''))
        elif hasattr(entry, 'summary_detail'):
            content = clean_html_content(entry.summary_detail.value)

        # If content is empty but description exists, use description
        if not content and description:
            content = description

        results.append({
            'title': title,
            'link': link,
            'description': description,
            'pub_date': pub_date,
            'content': content
        })

    return results

# Extract data from the RSS feed URL found in the snippet
rss_url = "https://www.webdo.tn/fr/feed/"
extracted_data = extract_rss_feed_data(rss_url)

# Print the extracted data without truncation
for i, item in enumerate(extracted_data, 1):
    print(f"=== Item {i} ===")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("\n" + "="*80 + "\n")

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities and clean extra whitespace
    clean_text = html.unescape(clean_text)
    clean_text = ' '.join(clean_text.split())

    return clean_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract fields with fallbacks for missing data
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))

        # Handle pubDate with different possible field names
        pub_date = entry.get('published', entry.get('pubDate', entry.get('updated', '')))
        pub_date = clean_html_content(pub_date)

        # Handle content - try different possible content fields
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)
        elif hasattr(entry, 'description'):
            content = clean_html_content(entry.description)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

# RSS URL from the snippet
rss_url = "https://www.babnet.net/feed.php"

# Parse the feed
parsed_data = parse_rss_feed(rss_url)

# Print the results without truncation
for i, item in enumerate(parsed_data, 1):
    print(f"Item {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)
    print()

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean the text using BeautifulSoup"""
    if not text:
        return ""

    # Parse with BeautifulSoup and get clean text
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Clean up any remaining HTML entities and extra spaces
    clean_text = re.sub(r'\s+', ' ', clean_text)
    clean_text = clean_text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    clean_text = clean_text.replace('&quot;', '"').replace('&#039;', "'")

    return clean_text.strip()

def extract_rss_feed(rss_url):
    """Extract and parse RSS feed content"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract fields
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Try to get content from different possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)
        else:
            content = description  # Fallback to description

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

# RSS URL from the snippet
rss_url = "https://www.nessma.tv/fr/rss/news/7"

# Extract and process the feed
feed_data = extract_rss_feed(rss_url)

# Print the results without truncation
for i, item in enumerate(feed_data, 1):
    print(f"Item {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)
    print()

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Handle case where text might be a list
    if isinstance(text, list):
        # If it's a list, take the first element or join them
        if text and isinstance(text[0], dict) and 'value' in text[0]:
            text = text[0]['value']
        else:
            text = ' '.join([str(item) for item in text])

    # Handle case where text might be a dictionary
    if isinstance(text, dict):
        if 'value' in text:
            text = text['value']
        else:
            text = str(text)

    # Parse with BeautifulSoup and get text
    soup = BeautifulSoup(str(text), 'html.parser')
    cleaned_text = soup.get_text(separator=' ', strip=True)

    # Clean up any remaining HTML entities and extra whitespace
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    return cleaned_text

def get_field_value(entry, field_names):
    """Safely get field value from entry, handling lists, dicts and multiple field names"""
    for field_name in field_names:
        if field_name in entry:
            value = entry[field_name]

            # Handle lists
            if isinstance(value, list):
                if value:
                    # If list contains dictionaries with 'value' key
                    if isinstance(value[0], dict) and 'value' in value[0]:
                        return value[0]['value']
                    else:
                        return ' '.join([str(item) for item in value])
                else:
                    return ""

            # Handle dictionaries
            elif isinstance(value, dict):
                if 'value' in value:
                    return value['value']
                else:
                    return str(value)

            # Handle strings and other types
            else:
                return value
    return ""

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract fields with proper handling for lists and dictionaries
        title = clean_html_content(get_field_value(entry, ['title']))
        link = get_field_value(entry, ['link'])

        # Get description from multiple possible fields
        description = clean_html_content(get_field_value(entry, ['description', 'summary', 'subtitle']))

        # Get content from multiple possible fields - handle content:encoded specifically
        content = ""
        if 'content' in entry:
            content = clean_html_content(entry['content'])
        elif 'content:encoded' in entry:
            content = clean_html_content(entry['content:encoded'])
        else:
            content = description  # Fallback

        # Get publication date from multiple possible fields
        pub_date = get_field_value(entry, ['published', 'pubDate', 'dc:date', 'updated'])

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

def main():
    # The actual RSS feed URL
    rss_url = "https://www.tunisienumerique.com/feed-actualites-tunisie.xml"

    print(f"Parsing RSS feed from: {rss_url}")
    print("=" * 100)

    try:
        # Parse the feed
        parsed_data = parse_rss_feed(rss_url)

        print(f"Found {len(parsed_data)} items in the RSS feed\n")

        # Print the results without truncation
        for i, item in enumerate(parsed_data, 1):
            print(f"ITEM {i}:")
            print(f"Title: {item['title']}")
            print(f"Link: {item['link']}")
            print(f"Description: {item['description']}")
            print(f"Publication Date: {item['pub_date']}")
            print(f"Content: {item['content']}")
            print("-" * 100)
            print()

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if text is None:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Clean up any remaining HTML entities and special characters
    clean_text = re.sub(r'\s+', ' ', clean_text)  # Replace multiple spaces with single space
    clean_text = clean_text.replace('&#8217;', "'")  # Replace HTML apostrophe
    clean_text = clean_text.replace('&amp;', '&')  # Replace HTML ampersand
    clean_text = clean_text.replace('&lt;', '<')  # Replace HTML less than
    clean_text = clean_text.replace('&gt;', '>')  # Replace HTML greater than
    clean_text = clean_text.replace('&quot;', '"')  # Replace HTML quotes
    clean_text = clean_text.replace('&#8230;', '...')  # Replace HTML ellipsis

    return clean_text.strip()

def extract_rss_feed_data(rss_url):
    """Extract and parse RSS feed data"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    # Check if feed was parsed successfully
    if feed.bozo:
        print(f"Error parsing feed: {feed.bozo_exception}")
        return []

    extracted_data = []

    # Extract data from each item
    for entry in feed.entries:
        # Get description content
        description_content = clean_html_content(entry.get('description', ''))

        # Get main content (if available)
        main_content = ''
        if entry.get('content'):
            main_content = clean_html_content(entry.get('content', [{}])[0].get('value', ''))

        # Combine description with main content if both exist
        full_content = description_content
        if main_content and main_content != description_content:
            full_content = f"{description_content} {main_content}"

        item_data = {
            "title": clean_html_content(entry.get('title', '')),
            "link": entry.get('link', ''),
            "description": description_content,
            "pub_date": entry.get('published', entry.get('pubDate', '')),
            "content": full_content
        }
        extracted_data.append(item_data)

    return extracted_data

# RSS URL from the snippet
rss_url = "https://africanmanager.com/feed/"

# Extract data from the RSS feed
feed_data = extract_rss_feed_data(rss_url)

# Print the extracted data without truncation
for i, item in enumerate(feed_data, 1):
    print(f"=== Item {i} ===")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("\n" + "="*50 + "\n")

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean text using BeautifulSoup"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Remove extra whitespace and normalize
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()

    return clean_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract and clean each field
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Get content - use description if content is empty
        content = clean_html_content(entry.get('content', [{}])[0].get('value', '')) if entry.get('content') else ''
        if not content:
            content = description

        item_data = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }
        results.append(item_data)

    return results

# RSS URL from the snippet
rss_url = "http://www.businessnews.com.tn/rss.xml"

# Parse the RSS feed
parsed_data = parse_rss_feed(rss_url)

# Print the extracted data without truncation
for i, item in enumerate(parsed_data, 1):
    print(f"ITEM {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    cleaned_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities and clean up whitespace
    cleaned_text = html.unescape(cleaned_text)
    cleaned_text = ' '.join(cleaned_text.split())

    return cleaned_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract and clean each field
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Try to get content from different possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)
        elif hasattr(entry, 'description'):
            content = clean_html_content(entry.description)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

# RSS URL from the snippet
rss_url = "https://www.radiotataouine.tn/articles/rss"

# Parse the feed
parsed_data = parse_rss_feed(rss_url)

# Print the results without truncation
for i, item in enumerate(parsed_data, 1):
    print(f"Item {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)
    print()

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities and clean up whitespace
    clean_text = html.unescape(clean_text)
    clean_text = ' '.join(clean_text.split())

    return clean_text

def parse_rss_feed(url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    results = []

    for entry in feed.entries:
        # Extract fields with fallbacks
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))

        # Handle publication date (prefer published, then updated, then current date)
        pub_date = entry.get('published', entry.get('updated', ''))
        pub_date = clean_html_content(pub_date)

        # Extract content - try multiple possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)
        elif hasattr(entry, 'description'):
            content = clean_html_content(entry.description)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

# Extract RSS URL from the snippet and parse
rss_url = "https://www.radiogafsa.tn/articles/rss"
parsed_data = parse_rss_feed(rss_url)

# Print the results without truncation
for i, item in enumerate(parsed_data, 1):
    print(f"Item {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)
    print()

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_feed_data(rss_url):
    """
    Extract and clean RSS feed data from the given URL
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = get_clean_text(entry.get('title', ''))
        link = entry.get('link', '')
        description = get_clean_text(entry.get('description', ''))
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Extract content - try multiple possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = get_clean_text(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = get_clean_text(entry.summary)
        elif hasattr(entry, 'description'):
            content = get_clean_text(entry.description)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

def get_clean_text(html_content):
    """
    Clean HTML content and extract plain text using BeautifulSoup
    """
    if not html_content:
        return ""

    # Decode HTML entities first
    decoded_content = html.unescape(html_content)

    # Use BeautifulSoup to extract text and remove HTML tags
    soup = BeautifulSoup(decoded_content, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Remove extra whitespace
    clean_text = ' '.join(clean_text.split())

    return clean_text

def print_results(results):
    """
    Print the extracted results without truncation
    """
    for i, result in enumerate(results, 1):
        print(f"=== Item {i} ===")
        print(f"Title: {result['title']}")
        print(f"Link: {result['link']}")
        print(f"Description: {result['description']}")
        print(f"Publication Date: {result['pub_date']}")
        print(f"Content: {result['content']}")
        print("-" * 80)
        print()

# RSS URL from the snippet
rss_url = "https://www.radiokef.tn/articles/rss"

# Extract and process the RSS feed
try:
    extracted_data = extract_rss_feed_data(rss_url)

    if extracted_data:
        print_results(extracted_data)
    else:
        print("No data extracted from the RSS feed.")

except Exception as e:
    print(f"Error processing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean unwanted content using BeautifulSoup"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Remove extra whitespace and clean up
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()

    return clean_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    extracted_data = []

    for entry in feed.entries:
        # Extract fields with fallbacks for missing data
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Extract content from multiple possible sources
        content_text = ''

        # First try to get description
        description = clean_html_content(entry.get('description', ''))
        if description:
            content_text = description

        # If no description or it's empty, try other content fields
        if not content_text:
            if hasattr(entry, 'content'):
                content_text = clean_html_content(entry.content[0].value if entry.content else '')
            elif hasattr(entry, 'summary'):
                content_text = clean_html_content(entry.summary)

        # If still no content, use title as fallback
        if not content_text:
            content_text = title

        # Ensure both description and content have the same text
        description = content_text

        # Create dictionary with extracted data
        item_data = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content_text
        }

        extracted_data.append(item_data)

    return extracted_data

# RSS URL from the snippet
rss_url = "https://www.rtci.tn/articles/rss"

# Parse the feed
try:
    results = parse_rss_feed(rss_url)

    # Print the extracted data without truncation
    for i, item in enumerate(results, 1):
        print(f"=== Item {i} ===")
        print(f"Title: {item['title']}")
        print(f"Link: {item['link']}")
        print(f"Description: {item['description']}")
        print(f"Publication Date: {item['pub_date']}")
        print(f"Content: {item['content']}")
        print("\n" + "-" * 80 + "\n")

except Exception as e:
    print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_feed_content(rss_url):
    """
    Extract and clean RSS feed content from the given URL
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = get_clean_text(entry.get('title', ''))
        link = entry.get('link', '')
        description = get_clean_text(entry.get('description', ''))
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Extract content - combine all available content sources
        content_parts = []

        # Add description content first
        if description and description != title:
            content_parts.append(description)

        # Add other content sources if available
        if hasattr(entry, 'content'):
            content_text = get_clean_text(entry.content[0].value)
            if content_text and content_text != description and content_text != title:
                content_parts.append(content_text)

        elif hasattr(entry, 'content_encoded'):
            content_text = get_clean_text(entry.content_encoded)
            if content_text and content_text != description and content_text != title:
                content_parts.append(content_text)

        elif hasattr(entry, 'summary_detail') and hasattr(entry.summary_detail, 'value'):
            content_text = get_clean_text(entry.summary_detail.value)
            if content_text and content_text != description and content_text != title:
                content_parts.append(content_text)

        elif hasattr(entry, 'summary'):
            content_text = get_clean_text(entry.summary)
            if content_text and content_text != description and content_text != title:
                content_parts.append(content_text)

        # Combine all content parts
        if content_parts:
            content = ' '.join(content_parts)
        else:
            content = description  # Fallback to description

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

def get_clean_text(html_content):
    """
    Clean HTML content and extract plain text using BeautifulSoup
    """
    if not html_content:
        return ""

    # Decode HTML entities first
    decoded_content = html.unescape(str(html_content))

    # Parse with BeautifulSoup and get text
    soup = BeautifulSoup(decoded_content, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Remove extra whitespace
    clean_text = ' '.join(clean_text.split())

    return clean_text

# Example usage with the RSS URL from your snippet
if __name__ == "__main__":
    rss_url = "https://www.radiomonastir.tn/articles/rss"

    try:
        entries = extract_rss_feed_content(rss_url)

        for i, entry in enumerate(entries, 1):
            print(f"Entry {i}:")
            print(f"Title: {entry['title']}")
            print(f"Link: {entry['link']}")
            print(f"Description: {entry['description']}")
            print(f"Publication Date: {entry['pub_date']}")
            print(f"Content: {entry['content']}")
            print("-" * 80)
            print()

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Unescape HTML entities
    clean_text = html.unescape(clean_text)

    return clean_text

def parse_rss_feed(url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    results = []

    for entry in feed.entries:
        # Extract fields with fallbacks
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))

        # Handle publication date (prefer published, then updated, then current date)
        pub_date = entry.get('published', '') or entry.get('updated', '') or entry.get('date', '')

        # Handle content - try multiple possible content fields and include description
        content_parts = []

        # First try to get main content
        if hasattr(entry, 'content') and entry.content:
            content_parts.append(clean_html_content(entry.content[0].value))
        elif hasattr(entry, 'summary'):
            content_parts.append(clean_html_content(entry.summary))

        # Always include description in content
        if description and description not in content_parts:
            content_parts.append(description)

        # Combine all content parts
        content = ' '.join(content_parts).strip()

        # If no content was found from other sources, use description as fallback
        if not content and description:
            content = description

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

# Example usage with the provided RSS URL
if __name__ == "__main__":
    rss_url = "https://www.radiojeunes.tn/articles/rss"

    try:
        articles = parse_rss_feed(rss_url)

        for i, article in enumerate(articles, 1):
            print(f"=== Article {i} ===")
            print(f"Title: {article['title']}")
            print(f"Link: {article['link']}")
            print(f"Description: {article['description']}")
            print(f"Publication Date: {article['pub_date']}")
            print(f"Content: {article['content']}")
            print("\n" + "-" * 80 + "\n")

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_content(rss_url):
    """
    Extract and clean content from RSS feed using feedparser and BeautifulSoup
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = getattr(entry, 'title', '')
        link = getattr(entry, 'link', '')
        description = getattr(entry, 'description', '')
        pub_date = getattr(entry, 'published', getattr(entry, 'pubDate', ''))

        # Extract content - try multiple possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = entry.content[0].value if entry.content else ''
        elif hasattr(entry, 'summary'):
            content = entry.summary

        # Combine content with description if both exist
        if content and description:
            combined_content = f"{description} {content}"
        elif content:
            combined_content = content
        else:
            combined_content = description

        # Clean HTML and unwanted strings using BeautifulSoup
        title_clean = clean_html_content(title)
        description_clean = clean_html_content(description)
        content_clean = clean_html_content(combined_content)

        # Create result dictionary
        result = {
            "title": title_clean,
            "link": link,
            "description": description_clean,
            "pub_date": pub_date,
            "content": content_clean
        }

        results.append(result)

    return results

def clean_html_content(text):
    """
    Clean HTML content and extract plain text using BeautifulSoup
    """
    if not text:
        return ""

    # Decode HTML entities first
    text = html.unescape(text)

    # Use BeautifulSoup to extract text and clean HTML tags
    soup = BeautifulSoup(text, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get clean text
    clean_text = soup.get_text()

    # Clean up whitespace
    clean_text = ' '.join(clean_text.split())

    return clean_text

def print_results(results):
    """
    Print the extracted results without truncation
    """
    for i, result in enumerate(results, 1):
        print(f"=== Item {i} ===")
        print(f"Title: {result['title']}")
        print(f"Link: {result['link']}")
        print(f"Description: {result['description']}")
        print(f"Publication Date: {result['pub_date']}")
        print(f"Content: {result['content']}")
        print("\n" + "-"*80 + "\n")

# Main execution
if __name__ == "__main__":
    rss_url = "https://www.radionationale.tn/articles/rss"

    try:
        results = extract_rss_content(rss_url)
        print_results(results)

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    clean_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities and clean up
    clean_text = html.unescape(clean_text)
    clean_text = ' '.join(clean_text.split())  # Normalize whitespace

    return clean_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract fields with fallbacks
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))

        # Handle publication date (try multiple possible fields)
        pub_date = entry.get('published', '') or entry.get('pubDate', '') or entry.get('updated', '')

        # Handle content (try multiple possible fields)
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)
        elif hasattr(entry, 'description'):
            content = clean_html_content(entry.description)

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

# Example usage
if __name__ == "__main__":
    rss_url = "https://www.radiosfax.tn/articles/rss"

    try:
        entries = parse_rss_feed(rss_url)

        for i, entry in enumerate(entries, 1):
            print(f"Entry {i}:")
            print(f"Title: {entry['title']}")
            print(f"Link: {entry['link']}")
            print(f"Description: {entry['description']}")
            print(f"Publication Date: {entry['pub_date']}")
            print(f"Content: {entry['content']}")
            print("-" * 80)
            print()

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Clean HTML content and extract plain text"""
    if not text:
        return ""

    # Parse HTML content
    soup = BeautifulSoup(text, 'html.parser')

    # Remove unwanted tags but preserve text content
    for element in soup(['script', 'style', 'iframe', 'noscript', 'header', 'footer', 'nav', 'aside']):
        element.decompose()

    # Get clean text
    clean_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities and normalize whitespace
    clean_text = html.unescape(clean_text)
    clean_text = ' '.join(clean_text.split())

    return clean_text

def parse_rss_feed(rss_url):
    """Parse RSS feed and extract required fields"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    # Process each item in the feed
    for entry in feed.entries:
        # Extract and clean each field
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')
        description = clean_html_content(entry.get('description', ''))

        # Handle publication date
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Handle content - try different possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)

        # Combine content with description if both exist
        if content and description:
            full_content = f"{content} {description}"
        elif content:
            full_content = content
        elif description:
            full_content = description
        else:
            full_content = ''

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": full_content
        }

        results.append(result)

    return results

# Extract RSS URL from the provided snippet
rss_url = "https://www.mosaiquefm.net/ar/rss"

# Parse the RSS feed
extracted_data = parse_rss_feed(rss_url)

# Print the extracted data without truncation
for i, item in enumerate(extracted_data, 1):
    print(f"Item {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)
    print()

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_content(rss_url):
    """
    Extract and clean content from RSS feed using feedparser and BeautifulSoup
    """
    # Parse the RSS feed
    print(f"Parsing RSS feed from: {rss_url}")
    feed = feedparser.parse(rss_url)

    # Check if feed was parsed successfully
    if feed.bozo and feed.bozo_exception:
        print(f"Error parsing RSS feed: {feed.bozo_exception}")
        return

    print(f"Found {len(feed.entries)} items in the feed\n")

    # Extract and process each item
    for i, item in enumerate(feed.entries, 1):
        print(f"Item {i}:")
        print("-" * 60)

        # Extract basic fields with fallbacks
        title = getattr(item, 'title', 'No title available')
        link = getattr(item, 'link', 'No link available')
        pub_date = getattr(item, 'published', getattr(item, 'pubDate', 'No date available'))

        # Extract and clean description
        description = getattr(item, 'description', '')
        cleaned_description = clean_html_content(description)

        # Extract content - try multiple possible fields
        content = ''
        if hasattr(item, 'content'):
            content = item.content[0].get('value', '') if item.content else ''
        elif hasattr(item, 'summary'):
            content = item.summary
        elif hasattr(item, 'description'):
            content = item.description

        cleaned_content = clean_html_content(content)

        # Print all extracted information
        print(f"Title: {title}")
        print(f"Link: {link}")
        print(f"Publication Date: {pub_date}")
        print(f"Description: {cleaned_description}")
        print(f"Content: {cleaned_content}")
        print("-" * 60)
        print("\n")

def clean_html_content(html_content):
    """
    Clean HTML content and extract plain text using BeautifulSoup
    """
    if not html_content:
        return "No content available"

    try:
        # Parse HTML content with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')

        # Remove unwanted tags but keep text content
        for unwanted_tag in soup(['script', 'style', 'img', 'br', 'link', 'meta', 'iframe', 'form', 'input', 'button']):
            unwanted_tag.decompose()

        # Remove all attributes from remaining tags
        for tag in soup.find_all(True):
            tag.attrs = {}

        # Get clean text and unescape HTML entities
        clean_text = soup.get_text(separator='\n', strip=True)
        clean_text = html.unescape(clean_text)

        # Remove extra whitespace and normalize line breaks
        lines = [line.strip() for line in clean_text.split('\n') if line.strip()]
        clean_text = '\n'.join(lines)

        return clean_text if clean_text else "No text content found after cleaning"

    except Exception as e:
        print(f"Error cleaning HTML content: {e}")
        # Fallback: return original content with basic cleaning
        return html.unescape(html_content).strip()

def main():
    """
    Main function to execute the RSS parsing
    """
    # The actual RSS feed URL provided
    rss_url = "https://www.jawharafm.net/ar/rss/showRss/88/1/1"

    print("Starting RSS Feed Extraction")
    print("=" * 60)

    try:
        extract_rss_content(rss_url)
    except Exception as e:
        print(f"An error occurred: {e}")

    print("Extraction completed!")

if __name__ == "__main__":
    main()

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and clean the text content"""
    if not text:
        return ""

    # Parse HTML content
    soup = BeautifulSoup(text, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Get text and clean it
    clean_text = soup.get_text()

    # Clean up whitespace and special characters
    clean_text = ' '.join(clean_text.split())
    clean_text = html.unescape(clean_text)

    return clean_text

def extract_rss_feed_data(url):
    """Extract and clean data from RSS feed"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    extracted_data = []

    for entry in feed.entries:
        # Extract basic fields
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')

        # Extract description and clean it
        description = clean_html_content(entry.get('description', ''))

        # Extract publication date
        pub_date = entry.get('published', '') or entry.get('pubDate', '')

        # Extract content - try different possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)
        elif hasattr(entry, 'description'):
            content = clean_html_content(entry.description)

        # For some feeds, content might be in different namespaces
        if not content and hasattr(entry, 'content_encoded'):
            content = clean_html_content(entry.content_encoded)

        item_data = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        extracted_data.append(item_data)

    return extracted_data

def main():
    # Extract the RSS URL from the provided snippet
    rss_url = "https://essahafa.tn/feed/"

    try:
        # Extract data from the RSS feed
        data = extract_rss_feed_data(rss_url)

        # Print the extracted data
        for i, item in enumerate(data, 1):
            print(f"Item {i}:")
            print(f"Title: {item['title']}")
            print(f"Link: {item['link']}")
            print(f"Description: {item['description']}")
            print(f"Publication Date: {item['pub_date']}")
            print(f"Content: {item['content']}")
            print("-" * 80)
            print()

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

if __name__ == "__main__":
    main()

import feedparser
from bs4 import BeautifulSoup
import html

def clean_html_content(text):
    """Remove HTML tags and decode HTML entities"""
    if not text:
        return ""

    # Parse with BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    cleaned_text = soup.get_text(separator=' ', strip=True)

    # Decode HTML entities
    cleaned_text = html.unescape(cleaned_text)

    return cleaned_text

def extract_rss_feed_data(url):
    """Extract and clean data from RSS feed"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    extracted_data = []

    for entry in feed.entries:
        # Extract and clean each field
        item_data = {
            "title": clean_html_content(entry.get('title', '')),
            "link": entry.get('link', ''),
            "description": clean_html_content(entry.get('description', '')),
            "pub_date": entry.get('published', entry.get('pubDate', '')),
            "content": clean_html_content(entry.get('content', [{}])[0].get('value', '') if entry.get('content') else '')
        }

        # If content is empty, try alternative content fields
        if not item_data['content']:
            item_data['content'] = clean_html_content(entry.get('summary', ''))

        extracted_data.append(item_data)

    return extracted_data

# RSS feed URL from the snippet
rss_url = "https://lapresse.tn/feed/"

# Extract data from the RSS feed
data = extract_rss_feed_data(rss_url)

# Print the extracted data without truncation
for i, item in enumerate(data, 1):
    print(f"Item {i}:")
    print(f"Title: {item['title']}")
    print(f"Link: {item['link']}")
    print(f"Description: {item['description']}")
    print(f"Publication Date: {item['pub_date']}")
    print(f"Content: {item['content']}")
    print("-" * 80)
    print()

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_content(rss_url):
    """
    Extract and clean RSS feed content from the given URL
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    # List to store extracted items
    extracted_items = []

    # Process each item in the feed
    for entry in feed.entries:
        item_data = {}

        # Extract title
        item_data['title'] = clean_html_content(entry.get('title', ''))

        # Extract link
        item_data['link'] = entry.get('link', '')

        # Extract description
        description = entry.get('description', '')
        if not description and hasattr(entry, 'summary'):
            description = entry.summary
        item_data['description'] = clean_html_content(description)

        # Extract publication date
        item_data['pub_date'] = entry.get('published', entry.get('pubDate', ''))

        # Extract content - try multiple possible fields
        content = ''
        if hasattr(entry, 'content'):
            content = entry.content[0].value if entry.content else ''
        elif hasattr(entry, 'content_encoded'):
            content = entry.content_encoded
        elif hasattr(entry, 'summary_detail'):
            content = entry.summary_detail.value

        item_data['content'] = clean_html_content(content)

        extracted_items.append(item_data)

    return extracted_items

def clean_html_content(text):
    """
    Clean HTML content using BeautifulSoup and return plain text
    """
    if not text:
        return ""

    # Decode HTML entities first
    decoded_text = html.unescape(text)

    # Parse with BeautifulSoup to extract text only
    soup = BeautifulSoup(decoded_text, 'html.parser')

    # Get clean text without HTML tags
    clean_text = soup.get_text(separator=' ', strip=False)

    # Clean up extra whitespace but preserve meaningful spacing
    clean_text = ' '.join(clean_text.split())

    return clean_text

def print_extracted_content(items):
    """
    Print extracted content without truncation
    """
    for i, item in enumerate(items, 1):
        print(f"=== Item {i} ===")
        print(f"Title: {item['title']}")
        print(f"Link: {item['link']}")
        print(f"Description: {item['description']}")
        print(f"Publication Date: {item['pub_date']}")
        print(f"Content: {item['content']}")
        print("\n" + "-"*80 + "\n")

# Main execution
if __name__ == "__main__":
    # Extract RSS URL from the provided snippet
    rss_url = "https://radiomedtunisie.com/feed/"

    try:
        # Extract content from the RSS feed
        extracted_items = extract_rss_content(rss_url)

        # Print the extracted content
        print_extracted_content(extracted_items)

    except Exception as e:
        print(f"Error processing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean unwanted content using BeautifulSoup"""
    if not text:
        return ""

    # Parse with BeautifulSoup
    soup = BeautifulSoup(text, 'html.parser')

    # Remove script, style, and other non-content elements
    for element in soup(["script", "style", "aside", "div", "span"]):
        element.decompose()

    # Get text and clean up
    clean_text = soup.get_text()

    # Remove extra whitespace and newlines
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()

    return clean_text

def remove_boilerplate_text(text):
    """Remove WordPress boilerplate text from the content"""
    if not text:
        return ""

    # Patterns to remove WordPress boilerplate
    patterns = [
        r'The post.*?appeared first on.*?\.?$',
        r'ظهر أولاً على.*?\.?$',
        r'المصدر:.*?\.?$',
        r'Source:.*?\.?$',
        r'أوازيس أف أم\.?$',
        r'The post.*?first on.*?\.?$'
    ]

    # Remove each pattern
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)

    # Clean up any trailing punctuation or whitespace
    text = re.sub(r'[\.\s]+$', '', text)
    text = text.strip()

    return text

def extract_article_content(content):
    """Extract the main article content by removing WordPress boilerplate"""
    if not content:
        return ""

    soup = BeautifulSoup(content, 'html.parser')

    # Remove common WordPress boilerplate patterns
    for element in soup.find_all(['p', 'div']):
        text = element.get_text().strip()
        # Remove elements that contain boilerplate text
        if any(phrase in text for phrase in ['The post', 'appeared first on', 'first on', 'Source:']):
            element.decompose()
        if any(phrase in text for phrase in ['أوازيس أف أم', 'المصدر:', 'ظهر أولاً على']):
            element.decompose()

    # Get clean text
    clean_content = soup.get_text()

    # Remove any remaining boilerplate
    clean_content = remove_boilerplate_text(clean_content)

    return clean_content

def extract_rss_feed_data(url):
    """Extract and clean RSS feed data"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    extracted_data = []

    for entry in feed.entries:
        # Extract basic fields
        title = entry.get('title', '')
        link = entry.get('link', '')

        # Handle description - clean HTML content and remove boilerplate
        description = entry.get('description', '')
        cleaned_description = clean_html_content(description)
        cleaned_description = remove_boilerplate_text(cleaned_description)

        # Handle publication date
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Handle content - try different possible content fields
        content = ''
        if hasattr(entry, 'content'):
            content = entry.content[0].value if entry.content else ''
        elif hasattr(entry, 'content_encoded'):
            content = entry.content_encoded
        elif hasattr(entry, 'summary'):
            content = entry.summary
        else:
            # Fallback to description if no content field found
            content = description

        # Extract actual article content by removing boilerplate
        cleaned_content = extract_article_content(content)

        # If content extraction failed, fall back to cleaned description
        if not cleaned_content or len(cleaned_content) < 20:
            cleaned_content = cleaned_description

        # Create data dictionary
        item_data = {
            "title": title,
            "link": link,
            "description": cleaned_description,
            "pub_date": pub_date,
            "content": cleaned_content
        }

        extracted_data.append(item_data)

    return extracted_data

def main():
    # RSS feed URL from the snippet
    rss_url = "https://oasis-fm.net/feed/"

    try:
        # Extract data from RSS feed
        data = extract_rss_feed_data(rss_url)

        # Print extracted data
        for i, item in enumerate(data, 1):
            print(f"Item {i}:")
            print(f"Title: {item['title']}")
            print(f"Link: {item['link']}")
            print(f"Description: {item['description']}")
            print(f"Publication Date: {item['pub_date']}")
            print(f"Content: {item['content']}")
            print("-" * 80)

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

if __name__ == "__main__":
    main()

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean the text using BeautifulSoup"""
    if not text:
        return ""

    # Parse with BeautifulSoup
    soup = BeautifulSoup(text, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
        script.decompose()

    # Get text and clean it
    text = soup.get_text()

    # Clean up whitespace and newlines
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def extract_feed_data(rss_url):
    """Extract and clean data from RSS feed"""
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    extracted_data = []

    for entry in feed.entries:
        # Extract basic fields
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')

        # Handle description (could be in different fields)
        description = clean_html_content(entry.get('description', ''))
        if not description:
            description = clean_html_content(entry.get('summary', ''))

        # Handle publication date
        pub_date = entry.get('published', '')
        if not pub_date:
            pub_date = entry.get('updated', '')
        if not pub_date:
            pub_date = entry.get('dc_date', '')

        # Handle content (could be in different fields)
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value if entry.content else '')
        elif hasattr(entry, 'content_encoded'):
            content = clean_html_content(entry.content_encoded)
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)

        # Create data dictionary
        item_data = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        extracted_data.append(item_data)

    return extracted_data

# Example usage with the provided RSS URL
if __name__ == "__main__":
    rss_url = "https://inkyfada.com/en/feed/"

    try:
        data = extract_feed_data(rss_url)

        for i, item in enumerate(data, 1):
            print(f"Item {i}:")
            print(f"Title: {item['title']}")
            print(f"Link: {item['link']}")
            print(f"Description: {item['description']}")
            print(f"Publication Date: {item['pub_date']}")
            print(f"Content: {item['content']}")
            print("-" * 80)

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import re

def clean_html_content(text):
    """Remove HTML tags and clean up whitespace"""
    if not text:
        return ""

    # Parse with BeautifulSoup
    soup = BeautifulSoup(text, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
        script.decompose()

    # Get text content
    text = soup.get_text()

    # Clean up whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def extract_rss_feed_data(url):
    """Extract and clean data from RSS feed"""
    # Parse the RSS feed
    feed = feedparser.parse(url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = clean_html_content(entry.get('title', ''))
        link = entry.get('link', '')

        # Handle description (may contain HTML)
        description = clean_html_content(entry.get('description', ''))

        # Handle publication date
        pub_date = entry.get('published', entry.get('pubDate', ''))

        # Handle content - try different possible content fields
        content = ''
        if hasattr(entry, 'content'):
            content = clean_html_content(entry.content[0].value)
        elif hasattr(entry, 'summary'):
            content = clean_html_content(entry.summary)
        elif hasattr(entry, 'description'):
            content = clean_html_content(entry.description)

        # For some feeds, content might be in other fields
        if not content:
            for key in entry.keys():
                if 'content' in key.lower() and isinstance(entry[key], str):
                    content = clean_html_content(entry[key])
                    break

        # Create result dictionary
        result = {
            "title": title,
            "link": link,
            "description": description,
            "pub_date": pub_date,
            "content": content
        }

        results.append(result)

    return results

def main():
    # Extract the URL from the provided snippet
    rss_url = "https://ftdes.net/feed/"

    try:
        # Extract data from RSS feed
        feed_data = extract_rss_feed_data(rss_url)

        # Print the extracted data
        for i, item in enumerate(feed_data, 1):
            print(f"=== Item {i} ===")
            print(f"Title: {item['title']}")
            print(f"Link: {item['link']}")
            print(f"Description: {item['description']}")
            print(f"Publication Date: {item['pub_date']}")
            print(f"Content: {item['content']}")
            print("\n" + "="*80 + "\n")

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

if __name__ == "__main__":
    main()

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_content(rss_url):
    """
    Extract and clean content from RSS feed using feedparser and BeautifulSoup
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = getattr(entry, 'title', '')
        link = getattr(entry, 'link', '')
        pub_date = getattr(entry, 'published', getattr(entry, 'pubDate', ''))

        # Extract and clean description
        description = getattr(entry, 'description', '')
        description_clean = clean_html_content(description)

        # Extract and clean content (try multiple possible fields)
        content = ''
        if hasattr(entry, 'content'):
            content = entry.content[0].value if entry.content else ''
        elif hasattr(entry, 'summary'):
            content = entry.summary
        content_clean = clean_html_content(content)

        # If content is empty but description has content, use description
        if not content_clean.strip() and description_clean.strip():
            content_clean = description_clean

        result = {
            "title": clean_text(title),
            "link": clean_text(link),
            "description": description_clean,
            "pub_date": clean_text(pub_date),
            "content": content_clean
        }

        results.append(result)

    return results

def clean_html_content(html_content):
    """
    Clean HTML content using BeautifulSoup to remove all HTML tags
    while preserving the text content
    """
    if not html_content:
        return ""

    # Unescape HTML entities first
    unescaped_content = html.unescape(html_content)

    # Parse with BeautifulSoup and extract text
    soup = BeautifulSoup(unescaped_content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style", "head", "title", "meta", "[document]"]):
        script.decompose()

    # Get text and clean up
    text = soup.get_text()

    # Clean up whitespace
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = ' '.join(chunk for chunk in chunks if chunk)

    return text

def clean_text(text):
    """
    Basic text cleaning (remove extra whitespace, unescape HTML entities)
    """
    if not text:
        return ""

    cleaned = html.unescape(text).strip()
    return cleaned

# Example usage with your RSS snippet URL
if __name__ == "__main__":
    # Replace this with the actual RSS URL from your snippet
    rss_url = "https://www.jawharafm.net/ar/rss/showRss/88/1/4"  # Assuming this is the base URL

    try:
        entries = extract_rss_content(rss_url)

        for i, entry in enumerate(entries, 1):
            print(f"=== Entry {i} ===")
            print(f"Title: {entry['title']}")
            print(f"Link: {entry['link']}")
            print(f"Description: {entry['description']}")
            print(f"Publication Date: {entry['pub_date']}")
            print(f"Content: {entry['content']}")
            print("=" * 50)
            print()

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

import feedparser
from bs4 import BeautifulSoup
import html

def extract_rss_content(rss_url):
    """
    Extract and clean content from RSS feed using feedparser and BeautifulSoup
    """
    # Parse the RSS feed
    feed = feedparser.parse(rss_url)

    results = []

    for entry in feed.entries:
        # Extract basic fields
        title = getattr(entry, 'title', '')
        link = getattr(entry, 'link', '')
        pub_date = getattr(entry, 'published', getattr(entry, 'pubDate', ''))

        # Extract and clean description
        description = getattr(entry, 'description', '')
        description_clean = clean_html_content(description)

        # Extract and clean content (try multiple possible fields)
        content = ''
        if hasattr(entry, 'content'):
            content = entry.content[0].value if entry.content else ''
        elif hasattr(entry, 'summary'):
            content = entry.summary
        content_clean = clean_html_content(content)

        # If content is empty but description has content, use description
        if not content_clean.strip() and description_clean.strip():
            content_clean = description_clean

        result = {
            "title": clean_text(title),
            "link": clean_text(link),
            "description": description_clean,
            "pub_date": clean_text(pub_date),
            "content": content_clean
        }

        results.append(result)

    return results

def clean_html_content(html_content):
    """
    Clean HTML content using BeautifulSoup to remove all HTML tags
    while preserving the text content
    """
    if not html_content:
        return ""

    # Unescape HTML entities first
    unescaped_content = html.unescape(html_content)

    # Parse with BeautifulSoup and extract text
    soup = BeautifulSoup(unescaped_content, 'html.parser')

    # Remove script and style elements
    for script in soup(["script", "style", "head", "title", "meta", "[document]"]):
        script.decompose()

    # Get text and clean up
    text = soup.get_text()

    # Clean up whitespace
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = ' '.join(chunk for chunk in chunks if chunk)

    return text

def clean_text(text):
    """
    Basic text cleaning (remove extra whitespace, unescape HTML entities)
    """
    if not text:
        return ""

    cleaned = html.unescape(text).strip()
    return cleaned

# Example usage with your RSS snippet URL
if __name__ == "__main__":
    # Replace this with the actual RSS URL from your snippet
    rss_url = "https://www.jawharafm.net/ar/rss/showRss/88/1/2"  # Assuming this is the base URL

    try:
        entries = extract_rss_content(rss_url)

        for i, entry in enumerate(entries, 1):
            print(f"=== Entry {i} ===")
            print(f"Title: {entry['title']}")
            print(f"Link: {entry['link']}")
            print(f"Description: {entry['description']}")
            print(f"Publication Date: {entry['pub_date']}")
            print(f"Content: {entry['content']}")
            print("=" * 50)
            print()

    except Exception as e:
        print(f"Error parsing RSS feed: {e}")

